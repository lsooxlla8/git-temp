{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'href'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[39mreturn\u001b[39;00m data\n\u001b[1;32m     35\u001b[0m \u001b[39m# Scrape the initial page\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m data \u001b[39m=\u001b[39m scrape_page(\u001b[39m\"\u001b[39;49m\u001b[39mhttps://neolurk.org/wiki/\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mD0\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39m9A\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mD0\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mB0\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mD1\u001b[39;49m\u001b[39m%82%\u001b[39;49;00m\u001b[39mD0\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mB5\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mD0\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mB3\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mD0\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mBE\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mD1\u001b[39;49m\u001b[39m%80%\u001b[39;49;00m\u001b[39mD0\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mB8\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mD1\u001b[39;49m\u001b[39m%8F\u001b[39;49;00m\u001b[39m:\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mD0\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39m9A\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mD0\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mBE\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mD0\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mBF\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mD0\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mB8\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mD0\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mBF\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mD0\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mB0\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mD1\u001b[39;49m\u001b[39m%81%\u001b[39;49;00m\u001b[39mD1\u001b[39;49m\u001b[39m%82%\u001b[39;49;00m\u001b[39mD0\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mB0:\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mD0\u001b[39;49m\u001b[39m%90%\u001b[39;49;00m\u001b[39mD1\u001b[39;49m\u001b[39m%80%\u001b[39;49;00m\u001b[39mD1\u001b[39;49m\u001b[39m%85%\u001b[39;49;00m\u001b[39mD0\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mB8\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mD0\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mB2\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     38\u001b[0m \u001b[39m# Save to JSON\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mparsed_data.json\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf8\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "Cell \u001b[0;32mIn[14], line 21\u001b[0m, in \u001b[0;36mscrape_page\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     17\u001b[0m data \u001b[39m=\u001b[39m []\n\u001b[1;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m link \u001b[39min\u001b[39;00m links:\n\u001b[1;32m     20\u001b[0m     \u001b[39m# Construct the full URL of the link\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     link_url \u001b[39m=\u001b[39m base_url \u001b[39m+\u001b[39m link[\u001b[39m'\u001b[39;49m\u001b[39mhref\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m     23\u001b[0m     \u001b[39m# If the link leads to another page that needs to be scraped\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39msome condition based on the link URL\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     25\u001b[0m         \u001b[39m# Recursively scrape the linked page\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/bs4/element.py:1573\u001b[0m, in \u001b[0;36mTag.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1570\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[1;32m   1571\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"tag[key] returns the value of the 'key' attribute for the Tag,\u001b[39;00m\n\u001b[1;32m   1572\u001b[0m \u001b[39m    and throws an exception if it's not there.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1573\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattrs[key]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'href'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# Base URL\n",
    "base_url = \"https://neolurk.org/wiki/\"\n",
    "\n",
    "# A function to scrape a single page\n",
    "def scrape_page(url):\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "    # Find all links on the page\n",
    "    links = soup.find_all('a')\n",
    "\n",
    "    data = []\n",
    "    \n",
    "    for link in links:\n",
    "        # Construct the full URL of the link\n",
    "        link_url = base_url + link['href']\n",
    "\n",
    "        # If the link leads to another page that needs to be scraped\n",
    "        if \"some condition based on the link URL\":\n",
    "            # Recursively scrape the linked page\n",
    "            data.extend(scrape_page(link_url))\n",
    "        else:\n",
    "            # Otherwise, parse the text on the page\n",
    "            parsed_text = \"the parsed text\"  # Use the appropriate BeautifulSoup methods here\n",
    "            item = {\"prompt\": \"напиши пасту\", \"completion\": parsed_text}\n",
    "            data.append(item)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Scrape the initial page\n",
    "data = scrape_page(\"https://neolurk.org/wiki/%D0%9A%D0%B0%D1%82%D0%B5%D0%B3%D0%BE%D1%80%D0%B8%D1%8F:%D0%9A%D0%BE%D0%BF%D0%B8%D0%BF%D0%B0%D1%81%D1%82%D0%B0:%D0%90%D1%80%D1%85%D0%B8%D0%B2\")\n",
    "\n",
    "# Save to JSON\n",
    "with open('parsed_data.json', 'w', encoding='utf8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install beautifulsoup4\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
